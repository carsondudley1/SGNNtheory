{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SkKtlF9pG0F",
        "outputId": "d23f10a4-7109-4454-b723-6adb7404ec29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Bayes] RMSE to true θ: 0.0271\n",
            "[Step 828928] SGNN→Bayes RMSE = 0.0845, SGNN→θ RMSE = 0.0325, Bayes→θ RMSE = 0.0269\n",
            "[Step 1657856] SGNN→Bayes RMSE = 0.0527, SGNN→θ RMSE = 0.0243, Bayes→θ RMSE = 0.0269\n",
            "[Step 2486784] SGNN→Bayes RMSE = 0.0538, SGNN→θ RMSE = 0.0237, Bayes→θ RMSE = 0.0269\n",
            "[Step 3315712] SGNN→Bayes RMSE = 0.0519, SGNN→θ RMSE = 0.0232, Bayes→θ RMSE = 0.0269\n",
            "[Step 4144640] SGNN→Bayes RMSE = 0.0526, SGNN→θ RMSE = 0.0228, Bayes→θ RMSE = 0.0269\n",
            "[Step 4973568] SGNN→Bayes RMSE = 0.0509, SGNN→θ RMSE = 0.0226, Bayes→θ RMSE = 0.0269\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "\n",
        "# ==============================\n",
        "# Simulator\n",
        "# ==============================\n",
        "\n",
        "def sample_theta(n, a=0.5, b=1.5):\n",
        "    return np.random.uniform(low=a, high=b, size=(n, 2))\n",
        "\n",
        "def simulate(theta, T=10, sigma=0.05):\n",
        "    X = []\n",
        "    for alpha, beta in theta:\n",
        "        A = np.array([[alpha, 0], [0, beta]])\n",
        "        x_t = np.array([1.0, 1.0])\n",
        "        traj = []\n",
        "        for _ in range(T):\n",
        "            x_t = A @ x_t + np.random.normal(0, sigma, size=2)\n",
        "            traj.append(x_t.copy())\n",
        "        X.append(np.array(traj).flatten())\n",
        "    return np.array(X)\n",
        "\n",
        "# ==============================\n",
        "# Bayes-optimal predictor\n",
        "# ==============================\n",
        "\n",
        "def compute_bayes_optimal_predictor(x_query, x_mc, theta_mc, k=50):\n",
        "    nn = NearestNeighbors(n_neighbors=k).fit(x_mc)\n",
        "    dists, indices = nn.kneighbors(x_query)\n",
        "    median_dist = np.median(dists, axis=1, keepdims=True)\n",
        "    adaptive_sigma = 0.5 * median_dist + 1e-8\n",
        "    weights = np.exp(-dists**2 / (2 * adaptive_sigma**2))\n",
        "    weights /= np.sum(weights, axis=1, keepdims=True)\n",
        "    theta_neighbors = theta_mc[indices]\n",
        "    return np.einsum('ij,ijk->ik', weights, theta_neighbors)\n",
        "\n",
        "# ==============================\n",
        "# Normalizer\n",
        "# ==============================\n",
        "\n",
        "class Normalizer:\n",
        "    def __init__(self):\n",
        "        self.x_scaler = StandardScaler()\n",
        "        self.y_scaler = StandardScaler()\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.x_scaler.fit(x)\n",
        "        self.y_scaler.fit(y)\n",
        "\n",
        "    def transform(self, x, y):\n",
        "        return self.x_scaler.transform(x), self.y_scaler.transform(y)\n",
        "\n",
        "    def transform_x(self, x):\n",
        "        return self.x_scaler.transform(x)\n",
        "\n",
        "    def inverse_y(self, y):\n",
        "        return self.y_scaler.inverse_transform(y)\n",
        "\n",
        "# ==============================\n",
        "# SGNN Model\n",
        "# ==============================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim, dropout=0.05):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.LayerNorm(dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class SGNN(nn.Module):\n",
        "    def __init__(self, input_dim=20, hidden_dim=1024, n_blocks=8):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim) for _ in range(n_blocks)])\n",
        "        self.out_proj = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.blocks(x)\n",
        "        return self.out_proj(x)\n",
        "\n",
        "# ==============================\n",
        "# Main Training + Evaluation Loop\n",
        "# ==============================\n",
        "\n",
        "# Reference set for Bayes estimator\n",
        "theta_ref = sample_theta(10000)\n",
        "x_ref = simulate(theta_ref)\n",
        "\n",
        "theta_val = sample_theta(1000)\n",
        "x_val = simulate(theta_val)\n",
        "\n",
        "normalizer = Normalizer()\n",
        "normalizer.fit(x_ref, theta_ref)\n",
        "x_ref_norm, theta_ref_norm = normalizer.transform(x_ref, theta_ref)\n",
        "x_val_norm = normalizer.transform_x(x_val)\n",
        "theta_val_norm = normalizer.y_scaler.transform(theta_val)\n",
        "\n",
        "# Bayes prediction\n",
        "bayes_y_val_norm = compute_bayes_optimal_predictor(x_val_norm, x_ref_norm, theta_ref_norm)\n",
        "bayes_y_val = normalizer.inverse_y(bayes_y_val_norm)\n",
        "bayes_rmse = np.sqrt(mean_squared_error(bayes_y_val, theta_val))\n",
        "print(f\"[Bayes] RMSE to true θ: {bayes_rmse:.4f}\")\n",
        "\n",
        "# SGNN training on 5M\n",
        "theta_train = sample_theta(5_000_000)\n",
        "x_train = simulate(theta_train)\n",
        "\n",
        "x_tr, x_holdout, y_tr, y_holdout = train_test_split(x_train, theta_train, test_size=0.005)\n",
        "normalizer = Normalizer()\n",
        "normalizer.fit(x_tr, y_tr)\n",
        "x_tr_norm, y_tr_norm = normalizer.transform(x_tr, y_tr)\n",
        "x_hold_norm = normalizer.transform_x(x_holdout)\n",
        "y_hold_bayes_norm = compute_bayes_optimal_predictor(x_hold_norm, x_ref_norm, theta_ref_norm)\n",
        "y_hold_bayes = normalizer.inverse_y(y_hold_bayes_norm)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x_train_tensor = torch.tensor(x_tr_norm, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_tr_norm, dtype=torch.float32).to(device)\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "model = SGNN(input_dim=x_tr.shape[1]).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader))\n",
        "\n",
        "# Tracking\n",
        "mse_to_bayes_list = []\n",
        "sgnn_vs_true_rmse_list = []\n",
        "bayes_vs_true_rmse_list = []\n",
        "training_steps = []\n",
        "\n",
        "for batch_idx, (xb, yb) in enumerate(train_loader):\n",
        "    model.train()\n",
        "    preds = model(xb)\n",
        "    loss = loss_fn(preds, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    if (batch_idx + 1) % (len(train_loader) // 6) == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_preds = model(torch.tensor(x_hold_norm, dtype=torch.float32).to(device))\n",
        "            val_preds_np = val_preds.cpu().numpy()\n",
        "            val_preds_denorm = normalizer.inverse_y(val_preds_np)\n",
        "\n",
        "            rmse_to_bayes = np.sqrt(mean_squared_error(\n",
        "                y_hold_bayes_norm, normalizer.y_scaler.transform(val_preds_denorm)))\n",
        "            sgnn_vs_true_rmse = np.sqrt(mean_squared_error(val_preds_denorm, y_holdout))\n",
        "            bayes_vs_true_rmse = np.sqrt(mean_squared_error(y_hold_bayes, y_holdout))\n",
        "            step_count = (batch_idx + 1) * 512  # samples seen\n",
        "\n",
        "            print(f\"[Step {step_count}] \"\n",
        "                  f\"SGNN→Bayes RMSE = {rmse_to_bayes:.4f}, \"\n",
        "                  f\"SGNN→θ RMSE = {sgnn_vs_true_rmse:.4f}, \"\n",
        "                  f\"Bayes→θ RMSE = {bayes_vs_true_rmse:.4f}\")\n",
        "\n",
        "            mse_to_bayes_list.append(rmse_to_bayes)\n",
        "            sgnn_vs_true_rmse_list.append(sgnn_vs_true_rmse)\n",
        "            bayes_vs_true_rmse_list.append(bayes_vs_true_rmse)\n",
        "            training_steps.append(step_count)\n",
        "\n",
        "# === Plotting ===\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.size\": 14,\n",
        "    \"figure.dpi\": 300,\n",
        "    \"text.usetex\": False\n",
        "})\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharex=False)\n",
        "\n",
        "axs[0].plot(training_steps, mse_to_bayes_list, label='SGNN → Bayes RMSE', marker='o', linewidth=2)\n",
        "axs[0].set_title('Convergence to Bayes Predictor', fontsize=14)\n",
        "axs[0].set_xlabel('Training Samples Seen', fontsize=13)\n",
        "axs[0].set_ylabel('MSE to Bayes Predictions', fontsize=13)\n",
        "axs[0].grid(True)\n",
        "axs[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "axs[1].plot(training_steps, sgnn_vs_true_rmse_list, label='SGNN → θ RMSE', marker='o', linewidth=2)\n",
        "axs[1].axhline(y=bayes_rmse, color='green', linestyle='--', linewidth=2, label='Bayes → θ RMSE')\n",
        "axs[1].set_title('Parameter Estimation Accuracy', fontsize=14)\n",
        "axs[1].set_xlabel('Training Samples Seen', fontsize=13)\n",
        "axs[1].set_ylabel('RMSE to True θ', fontsize=13)\n",
        "axs[1].legend(fontsize=12)\n",
        "axs[1].grid(True)\n",
        "axs[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"sgnn_accuracy.pdf\", format=\"pdf\", bbox_inches='tight')\n",
        "plt.close()"
      ]
    }
  ]
}