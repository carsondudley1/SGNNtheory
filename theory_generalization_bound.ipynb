{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e75ac7-c738-48f5-83f8-cfd740903c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Simulator parameters\n",
    "d = 5\n",
    "A0 = torch.eye(d)\n",
    "noise_std = 0.1\n",
    "\n",
    "# Neural net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Simulate data from a given A\n",
    "def simulate(A, n=1000):\n",
    "    x = torch.randn(n, d)\n",
    "    noise = noise_std * torch.randn(n, d)\n",
    "    y = x @ A.T + noise\n",
    "    return x, y\n",
    "\n",
    "# Theoretical worst-case bound\n",
    "def theoretical_bound(empirical_loss, delta, sigma=noise_std, d=d):\n",
    "    return empirical_loss + 0.5 * delta * (d**0.5) / sigma\n",
    "\n",
    "# Empirical data-dependent bound\n",
    "def empirical_bound(empirical_loss, A0, A_star, X_test, sigma=noise_std):\n",
    "    delta_matrix = A_star - A0\n",
    "    mean_shift = (X_test @ delta_matrix.T).norm(dim=1).mean().item()\n",
    "    return empirical_loss + mean_shift / (2 * sigma)\n",
    "\n",
    "# Training and evaluation\n",
    "def train_and_eval(A_star):\n",
    "    X_train, y_train = simulate(A0, n=2000)\n",
    "    X_test, y_test = simulate(A_star, n=1000)\n",
    "\n",
    "    model = Net()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for _ in range(2000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model(X_train), y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = loss_fn(model(X_test), y_test).item()\n",
    "\n",
    "    return test_loss, X_test\n",
    "\n",
    "# Run across mismatch levels\n",
    "delta_vals = torch.linspace(0, 2, 20)\n",
    "empirical_losses = []\n",
    "theoretical_bounds = []\n",
    "empirical_bounds = []\n",
    "\n",
    "for delta in delta_vals:\n",
    "    # Construct A* = A0 + delta * U / ||U||\n",
    "    U = torch.randn(d, d)\n",
    "    U = U / torch.norm(U)\n",
    "    A_star = A0 + delta * U\n",
    "\n",
    "    test_loss, X_test = train_and_eval(A_star)\n",
    "    empirical_losses.append(test_loss)\n",
    "\n",
    "    # Theoretical worst-case bound\n",
    "    theoretical_bounds.append(theoretical_bound(test_loss, delta.item()))\n",
    "\n",
    "    # Empirical bound using actual ||Î”x||\n",
    "    empirical_bounds.append(empirical_bound(test_loss, A0, A_star, X_test))\n",
    "\n",
    "# Convert to numpy\n",
    "delta_vals = delta_vals.numpy()\n",
    "empirical_losses = np.array(empirical_losses)\n",
    "theoretical_bounds = np.array(theoretical_bounds)\n",
    "empirical_bounds = np.array(empirical_bounds)\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 14,\n",
    "    \"figure.dpi\": 300,\n",
    "    \"text.usetex\": False  # Set to True if you're using LaTeX installation\n",
    "})\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "# Left: theoretical bound\n",
    "axs[0].plot(delta_vals, empirical_losses, label='Empirical Test Loss', marker='o', linewidth=2)\n",
    "axs[0].plot(delta_vals, theoretical_bounds, '--', label='Worst-Case Bound', color='orange', linewidth=2)\n",
    "axs[0].set_title('Worst-Case Generalization Bound', fontsize=14)\n",
    "axs[0].set_xlabel(r'Simulator Mismatch $\\delta$', fontsize=13)\n",
    "axs[0].set_ylabel('Test MSE Loss', fontsize=13)\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].legend(fontsize=12)\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Right: empirical bound\n",
    "axs[1].plot(delta_vals, empirical_losses, label='Empirical Test Loss', marker='o', linewidth=2)\n",
    "axs[1].plot(delta_vals, empirical_bounds, '--', label='Empirical Bound', color='green', linewidth=2)\n",
    "axs[1].set_title('Data-Dependent Generalization Bound', fontsize=14)\n",
    "axs[1].set_xlabel(r'Simulator Mismatch $\\delta$', fontsize=13)\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].legend(fontsize=12)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Tight layout and save\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sgnn_generalization_bounds.pdf\", format=\"pdf\", bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53215ab2-edcc-4d85-a2ba-0f78a284be30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
